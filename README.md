# Transformer æ¨¡å‹å‰ªæå’Œé‡åŒ–è¯¾ç¨‹è®¾è®¡é¡¹ç›®

<div align="center">

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ä¸€ä¸ªå®Œæ•´çš„ Transformer æ¨¡å‹å‰ªæå’Œé‡åŒ–çš„è¯¾ç¨‹è®¾è®¡é¡¹ç›®ï¼ŒåŒ…å«ç†è®ºå®ç°ã€å®éªŒä»£ç å’Œè¯¦ç»†æ–‡æ¡£ã€‚

[åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§) â€¢
[å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) â€¢
[é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„) â€¢
[ä½¿ç”¨æ–‡æ¡£](#ä½¿ç”¨æ–‡æ¡£) â€¢
[å®éªŒç»“æœ](#å®éªŒç»“æœ)

</div>

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
  - [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
  - [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
  - [è¿è¡Œç¤ºä¾‹](#è¿è¡Œç¤ºä¾‹)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [ä½¿ç”¨æ–‡æ¡£](#ä½¿ç”¨æ–‡æ¡£)
- [æ ¸å¿ƒæ¨¡å—](#æ ¸å¿ƒæ¨¡å—)
- [å®éªŒæµç¨‹](#å®éªŒæµç¨‹)
- [å®éªŒç»“æœ](#å®éªŒç»“æœ)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)
- [è®¸å¯è¯](#è®¸å¯è¯)
- [è‡´è°¢](#è‡´è°¢)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Transformer æ¨¡å‹å‰ªæå’Œé‡åŒ–çš„æ•™å­¦å’Œç ”ç©¶é¡¹ç›®ï¼Œæ—¨åœ¨å¸®åŠ©å­¦ä¹ è€…æ·±å…¥ç†è§£æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå¹¶æä¾›å¯ç›´æ¥è¿è¡Œçš„å®éªŒä»£ç ã€‚

### æ ¸å¿ƒç›®æ ‡

1. **æ•™å­¦ç›®çš„**ï¼šæä¾›æ¸…æ™°çš„ä¸­æ–‡æ–‡æ¡£å’Œæ³¨é‡Šï¼Œå¸®åŠ©ç†è§£å‰ªæå’Œé‡åŒ–æŠ€æœ¯
2. **å®è·µå¯¼å‘**ï¼šåŒ…å«å®Œæ•´çš„å®éªŒè„šæœ¬ï¼Œå¯ç›´æ¥è¿è¡Œå’Œä¿®æ”¹
3. **å¯å¤ç°æ€§**ï¼šæä¾›è¯¦ç»†çš„é…ç½®æ–‡ä»¶å’Œå®éªŒæŠ¥å‘Šæ¨¡æ¿
4. **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•æ–°çš„æ–¹æ³•

### é€‚ç”¨åœºæ™¯

- ğŸ“ è¯¾ç¨‹è®¾è®¡å’Œæ¯•ä¸šè®¾è®¡
- ğŸ“š æ·±åº¦å­¦ä¹ æ¨¡å‹å‹ç¼©å­¦ä¹ 
- ğŸ”¬ æ¨¡å‹å‹ç¼©æŠ€æœ¯ç ”ç©¶
- ğŸ’¼ å®é™…é¡¹ç›®çš„æ¨¡å‹ä¼˜åŒ–

---

## âœ¨ åŠŸèƒ½ç‰¹æ€§

### æ¨¡å‹å‰ªæ

- âœ… **ç»“æ„åŒ–å‰ªæ**
  - æ³¨æ„åŠ›å¤´å‰ªæï¼ˆAttention Head Pruningï¼‰
  - FFN ç¥ç»å…ƒå‰ªæ
  - åŸºäºé‡è¦æ€§çš„å‰ªæç­–ç•¥

- âœ… **éç»“æ„åŒ–å‰ªæ**
  - å¹…åº¦å‰ªæï¼ˆMagnitude Pruningï¼‰
  - L1/L2 èŒƒæ•°å‰ªæ
  - å…¨å±€å’Œå±€éƒ¨å‰ªæç­–ç•¥
  - æ”¯æŒå¤šç§ç¨€ç–åº¦ï¼ˆ10% - 90%ï¼‰

- âœ… **æ¸è¿›å¼å‰ªæ**
  - è¿­ä»£å¼å‰ªæ
  - è‡ªåŠ¨ç¨€ç–åº¦è°ƒåº¦
  - å¾®è°ƒæ¢å¤æ€§èƒ½

### æ¨¡å‹é‡åŒ–

- âœ… **è®­ç»ƒåé‡åŒ– (PTQ)**
  - åŠ¨æ€é‡åŒ–ï¼ˆDynamic Quantizationï¼‰
  - é™æ€é‡åŒ–ï¼ˆStatic Quantizationï¼‰
  - INT8 é‡åŒ–æ”¯æŒ

- âœ… **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT)**
  - ä¼ªé‡åŒ–è®­ç»ƒ
  - INT8/FP16 æ”¯æŒ
  - æ€§èƒ½æ¢å¤è®­ç»ƒ

- âœ… **æ··åˆç²¾åº¦é‡åŒ–**
  - å±‚æ•æ„Ÿåº¦åˆ†æ
  - è‡ªåŠ¨æ··åˆç²¾åº¦é…ç½®
  - æ¨¡å‹å¤§å°ä¼°ç®—

### å·¥å…·å’Œè¯„ä¼°

- ğŸ“Š **æ€§èƒ½è¯„ä¼°**
  - å‡†ç¡®ç‡ã€F1 åˆ†æ•°ã€ç²¾ç¡®ç‡ã€å¬å›ç‡
  - æ¨¡å‹å¤§å°å’Œå‚æ•°ç»Ÿè®¡
  - æ¨ç†é€Ÿåº¦æµ‹é‡
  - ç¨€ç–åº¦è®¡ç®—

- ğŸ“ˆ **å¯è§†åŒ–å·¥å…·**
  - è®­ç»ƒæ›²çº¿ç»˜åˆ¶
  - æ€§èƒ½å¯¹æ¯”å›¾è¡¨
  - æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
  - ç¨€ç–åº¦ vs å‡†ç¡®ç‡æ›²çº¿

### å®éªŒæ”¯æŒ

- ğŸ§ª **å®Œæ•´çš„å®éªŒè„šæœ¬**
  - åŸºçº¿æ¨¡å‹è®­ç»ƒ
  - å‰ªæå®éªŒï¼ˆç»“æ„åŒ–ã€éç»“æ„åŒ–ã€æ¸è¿›å¼ï¼‰
  - é‡åŒ–å®éªŒï¼ˆPTQã€QATï¼‰
  - ç»„åˆå®éªŒï¼ˆå‰ªæ+é‡åŒ–ï¼‰
  - ç»“æœå¯¹æ¯”å’Œåˆ†æ

- ğŸ“ **è¯¦ç»†æ–‡æ¡£**
  - ç†è®ºåŸºç¡€è¯´æ˜ï¼ˆä¸­æ–‡ï¼‰
  - ä½¿ç”¨æŒ‡å—å’Œ API æ–‡æ¡£
  - å®éªŒæŠ¥å‘Šæ¨¡æ¿
  - Jupyter Notebook ç¤ºä¾‹

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.0+ (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)

### å®‰è£…æ­¥éª¤

1. **å…‹éš†é¡¹ç›®**

```bash
git clone https://github.com/kleene123/myclasswork.git
cd myclasswork
```

2. **å®‰è£…ä¾èµ–**

```bash
# ä½¿ç”¨ pip
pip install -r requirements.txt

# æˆ–ä½¿ç”¨å¼€å‘æ¨¡å¼å®‰è£…
pip install -e .
```

3. **éªŒè¯å®‰è£…**

```bash
python -c "import torch; import transformers; print('å®‰è£…æˆåŠŸï¼')"
```

### è¿è¡Œç¤ºä¾‹

#### 1. è®­ç»ƒåŸºçº¿æ¨¡å‹

```bash
python experiments/train_baseline.py
```

#### 2. è¿è¡Œå‰ªæå®éªŒ

```bash
# è¿è¡Œæ‰€æœ‰å‰ªæå®éªŒ
python experiments/experiment_pruning.py

# æˆ–è¿è¡Œç‰¹å®šå®éªŒ
python experiments/experiment_pruning.py --unstructured
```

#### 3. è¿è¡Œé‡åŒ–å®éªŒ

```bash
# è¿è¡ŒåŠ¨æ€é‡åŒ–
python experiments/experiment_quantization.py --dynamic
```

#### 4. æŸ¥çœ‹ç»“æœå¯¹æ¯”

```bash
python experiments/compare_results.py
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
transformer-pruning-quantization/
â”œâ”€â”€ README.md                      # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ requirements.txt               # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ setup.py                       # é¡¹ç›®å®‰è£…é…ç½®
â”‚
â”œâ”€â”€ configs/                       # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ base_config.yaml          # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ pruning_config.yaml       # å‰ªæé…ç½®
â”‚   â””â”€â”€ quantization_config.yaml  # é‡åŒ–é…ç½®
â”‚
â”œâ”€â”€ src/                          # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                   # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ transformer.py       # Transformer æ¨¡å‹
â”‚   â”‚   â””â”€â”€ bert_wrapper.py      # BERT å°è£…
â”‚   â”œâ”€â”€ pruning/                  # å‰ªææ¨¡å—
â”‚   â”‚   â”œâ”€â”€ structured_pruning.py    # ç»“æ„åŒ–å‰ªæ
â”‚   â”‚   â”œâ”€â”€ unstructured_pruning.py  # éç»“æ„åŒ–å‰ªæ
â”‚   â”‚   â””â”€â”€ progressive_pruning.py   # æ¸è¿›å¼å‰ªæ
â”‚   â”œâ”€â”€ quantization/             # é‡åŒ–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ ptq.py               # è®­ç»ƒåé‡åŒ–
â”‚   â”‚   â”œâ”€â”€ qat.py               # é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
â”‚   â”‚   â””â”€â”€ mixed_precision.py   # æ··åˆç²¾åº¦é‡åŒ–
â”‚   â”œâ”€â”€ utils/                    # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ data_loader.py       # æ•°æ®åŠ è½½
â”‚   â”‚   â”œâ”€â”€ metrics.py           # è¯„ä¼°æŒ‡æ ‡
â”‚   â”‚   â””â”€â”€ visualization.py     # å¯è§†åŒ–
â”‚   â””â”€â”€ training/                 # è®­ç»ƒæ¨¡å—
â”‚       â”œâ”€â”€ trainer.py           # è®­ç»ƒå™¨
â”‚       â””â”€â”€ evaluator.py         # è¯„ä¼°å™¨
â”‚
â”œâ”€â”€ experiments/                  # å®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ train_baseline.py        # è®­ç»ƒåŸºçº¿æ¨¡å‹
â”‚   â”œâ”€â”€ experiment_pruning.py    # å‰ªæå®éªŒ
â”‚   â”œâ”€â”€ experiment_quantization.py # é‡åŒ–å®éªŒ
â”‚   â”œâ”€â”€ experiment_combined.py   # ç»„åˆå®éªŒ
â”‚   â””â”€â”€ compare_results.py       # ç»“æœå¯¹æ¯”
â”‚
â”œâ”€â”€ docs/                         # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ ç†è®ºåŸºç¡€.md              # ç†è®ºè¯´æ˜
â”‚   â”œâ”€â”€ ä½¿ç”¨æŒ‡å—.md              # ä½¿ç”¨æ•™ç¨‹
â”‚   â””â”€â”€ å®éªŒæŠ¥å‘Šæ¨¡æ¿.md          # æŠ¥å‘Šæ¨¡æ¿
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_æ•°æ®æ¢ç´¢.ipynb
â”‚   â”œâ”€â”€ 02_å‰ªæå®éªŒ.ipynb
â”‚   â”œâ”€â”€ 03_é‡åŒ–å®éªŒ.ipynb
â”‚   â””â”€â”€ 04_ç»“æœå¯è§†åŒ–.ipynb
â”‚
â””â”€â”€ tests/                        # å•å…ƒæµ‹è¯•
    â”œâ”€â”€ test_pruning.py
    â””â”€â”€ test_quantization.py
```

---

## ğŸ“š ä½¿ç”¨æ–‡æ¡£

### è¯¦ç»†æ–‡æ¡£

- **[ç†è®ºåŸºç¡€](docs/ç†è®ºåŸºç¡€.md)** - Transformerã€å‰ªæã€é‡åŒ–çš„ç†è®ºçŸ¥è¯†
- **[ä½¿ç”¨æŒ‡å—](docs/ä½¿ç”¨æŒ‡å—.md)** - è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜å’Œ API æ–‡æ¡£
- **[å®éªŒæŠ¥å‘Šæ¨¡æ¿](docs/å®éªŒæŠ¥å‘Šæ¨¡æ¿.md)** - å®Œæ•´çš„å®éªŒæŠ¥å‘Šæ¨¡æ¿

### é…ç½®æ–‡ä»¶è¯´æ˜

#### åŸºç¡€é…ç½® (`configs/base_config.yaml`)

```yaml
model:
  name: "bert-base-uncased"  # é¢„è®­ç»ƒæ¨¡å‹åç§°
  num_labels: 2              # åˆ†ç±»ç±»åˆ«æ•°

training:
  batch_size: 32             # æ‰¹æ¬¡å¤§å°
  learning_rate: 2e-5        # å­¦ä¹ ç‡
  epochs: 3                  # è®­ç»ƒè½®æ•°
```

#### å‰ªæé…ç½® (`configs/pruning_config.yaml`)

```yaml
pruning:
  method: "structured"       # å‰ªææ–¹æ³•
  attention_head_pruning:
    enabled: true
    num_heads_to_prune: 4   # å‰ªæå¤´æ•°
  unstructured:
    sparsity_levels: [0.1, 0.3, 0.5, 0.7, 0.9]
```

---

## ğŸ§© æ ¸å¿ƒæ¨¡å—

### 1. æ¨¡å‹æ¨¡å—

```python
from src.models.bert_wrapper import BERTWrapper

# åˆå§‹åŒ– BERT æ¨¡å‹
model = BERTWrapper(
    model_name='bert-base-uncased',
    num_labels=2
)
```

### 2. å‰ªææ¨¡å—

```python
from src.pruning.unstructured_pruning import UnstructuredPruning

# åº”ç”¨éç»“æ„åŒ–å‰ªæ
pruner = UnstructuredPruning(model)
pruner.apply_pruning(sparsity=0.5)
```

### 3. é‡åŒ–æ¨¡å—

```python
from src.quantization.ptq import PostTrainingQuantization

# åº”ç”¨åŠ¨æ€é‡åŒ–
ptq = PostTrainingQuantization(model)
quantized_model = ptq.apply_dynamic_quantization()
```

### 4. è¯„ä¼°æ¨¡å—

```python
from src.training.evaluator import Evaluator

# è¯„ä¼°æ¨¡å‹
evaluator = Evaluator(model, device)
metrics = evaluator.evaluate(test_loader)
```

---

## ğŸ”¬ å®éªŒæµç¨‹

### æ ‡å‡†å®éªŒæµç¨‹

```mermaid
graph LR
    A[æ•°æ®å‡†å¤‡] --> B[è®­ç»ƒåŸºçº¿æ¨¡å‹]
    B --> C[æ¨¡å‹å‰ªæ]
    B --> D[æ¨¡å‹é‡åŒ–]
    C --> E[ç»„åˆä¼˜åŒ–]
    D --> E
    E --> F[æ€§èƒ½è¯„ä¼°]
    F --> G[ç»“æœå¯¹æ¯”]
```

### å®éªŒæ­¥éª¤

1. **å‡†å¤‡ç¯å¢ƒå’Œæ•°æ®**
   ```bash
   pip install -r requirements.txt
   ```

2. **è®­ç»ƒåŸºçº¿æ¨¡å‹**
   ```bash
   python experiments/train_baseline.py
   ```

3. **æ‰§è¡Œå‰ªæå®éªŒ**
   ```bash
   python experiments/experiment_pruning.py
   ```

4. **æ‰§è¡Œé‡åŒ–å®éªŒ**
   ```bash
   python experiments/experiment_quantization.py
   ```

5. **ç»„åˆä¼˜åŒ–å®éªŒ**
   ```bash
   python experiments/experiment_combined.py
   ```

6. **åˆ†æå’Œå¯¹æ¯”ç»“æœ**
   ```bash
   python experiments/compare_results.py
   ```

---

## ğŸ“Š å®éªŒç»“æœ

### å…¸å‹æ€§èƒ½æŒ‡æ ‡

| æ–¹æ³• | å‡†ç¡®ç‡ | æ¨¡å‹å¤§å° | å‹ç¼©æ¯” | æ¨ç†é€Ÿåº¦æå‡ |
|------|--------|----------|--------|-------------|
| BERT åŸºçº¿ | 92.5% | 440 MB | 1.0x | 1.0x |
| ç»“æ„åŒ–å‰ªæ (4å¤´) | 91.8% | 330 MB | 1.33x | 1.2x |
| éç»“æ„åŒ–å‰ªæ (50%) | 91.2% | 440 MB | 1.0x | 1.0x |
| åŠ¨æ€é‡åŒ– (INT8) | 92.0% | 110 MB | 4.0x | 2.5x |
| å‰ªæ+é‡åŒ– | 90.5% | 82 MB | 5.4x | 3.0x |

*æ³¨ï¼šå®é™…ç»“æœå–å†³äºå…·ä½“çš„æ•°æ®é›†å’Œä»»åŠ¡*

### æ€§èƒ½å¯è§†åŒ–

é¡¹ç›®ä¼šè‡ªåŠ¨ç”Ÿæˆä»¥ä¸‹å¯è§†åŒ–å›¾è¡¨ï¼š

- ğŸ“ˆ ç¨€ç–åº¦ vs å‡†ç¡®ç‡æ›²çº¿
- ğŸ“Š æ¨¡å‹å¤§å° vs å‡†ç¡®ç‡æ•£ç‚¹å›¾
- ğŸ¨ å„æ–¹æ³•æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
- ğŸ”¥ æ€§èƒ½çƒ­åŠ›å›¾

---

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### æ”¯æŒçš„æ¨¡å‹

- âœ… BERT (bert-base-uncased, bert-large-uncased)
- âœ… DistilBERT
- âœ… è‡ªå®šä¹‰ Transformer æ¨¡å‹

### æ”¯æŒçš„æ•°æ®é›†

- âœ… IMDB (æƒ…æ„Ÿåˆ†æ)
- âœ… SST-2 (GLUE benchmark)
- âœ… AG News (æ–°é—»åˆ†ç±»)
- âœ… è‡ªå®šä¹‰æ–‡æœ¬åˆ†ç±»æ•°æ®é›†

### æ”¯æŒçš„å‰ªææ–¹æ³•

- æ³¨æ„åŠ›å¤´å‰ªæ
- FFN ç¥ç»å…ƒå‰ªæ
- å¹…åº¦å‰ªæ
- L1/L2 èŒƒæ•°å‰ªæ
- æ¸è¿›å¼å‰ªæ

### æ”¯æŒçš„é‡åŒ–æ–¹æ³•

- åŠ¨æ€é‡åŒ– (INT8)
- é™æ€é‡åŒ– (INT8)
- é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
- æ··åˆç²¾åº¦é‡åŒ– (INT8/FP16)

---

## â“ å¸¸è§é—®é¢˜

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç¨€ç–åº¦ï¼Ÿ

**A**: å»ºè®®ä» 30% å¼€å§‹ï¼Œé€æ­¥å¢åŠ åˆ° 50-70%ã€‚ä¸åŒä»»åŠ¡çš„æœ€ä¼˜ç¨€ç–åº¦ä¸åŒï¼Œéœ€è¦å®éªŒç¡®å®šã€‚

### Q: é‡åŒ–ä¼šæŸå¤±å¤šå°‘ç²¾åº¦ï¼Ÿ

**A**: é€šå¸¸åŠ¨æ€é‡åŒ–çš„ç²¾åº¦æŸå¤±åœ¨ 0.5-1% ä¹‹é—´ï¼ŒQAT å¯ä»¥è¿›ä¸€æ­¥å‡å°æŸå¤±ã€‚

### Q: å¦‚ä½•åœ¨ CPU ä¸Šè¿è¡Œï¼Ÿ

**A**: ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹è®¾å¤‡ã€‚åœ¨æ²¡æœ‰ GPU æ—¶ä¼šä½¿ç”¨ CPUï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ã€‚

### Q: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ï¼Ÿ

**A**: å‚è€ƒ `src/utils/data_loader.py` ä¸­çš„ `TextClassificationDataset` ç±»ï¼Œå‡†å¤‡è‡ªå·±çš„æ•°æ®ã€‚

æ›´å¤šé—®é¢˜è¯·æŸ¥çœ‹ [ä½¿ç”¨æŒ‡å—](docs/ä½¿ç”¨æŒ‡å—.md#å¸¸è§é—®é¢˜)ã€‚

---

## ğŸ§ª è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/

# è¿è¡Œå‰ªææµ‹è¯•
python -m pytest tests/test_pruning.py

# è¿è¡Œé‡åŒ–æµ‹è¯•
python -m pytest tests/test_quantization.py
```

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

### ä»£ç è§„èŒƒ

- éµå¾ª PEP 8 ä»£ç é£æ ¼
- æ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šå’Œ docstring
- ä¸ºæ–°åŠŸèƒ½ç¼–å†™æµ‹è¯•
- æ›´æ–°ç›¸å…³æ–‡æ¡£

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è°¢

### å‚è€ƒè®ºæ–‡

- Vaswani et al., "Attention is All You Need", NeurIPS 2017
- Michel et al., "Are Sixteen Heads Really Better than One?", NeurIPS 2019
- Jacob et al., "Quantization and Training of Neural Networks", CVPR 2018
- Frankle & Carbin, "The Lottery Ticket Hypothesis", ICLR 2019

### å¼€æºé¡¹ç›®

- [PyTorch](https://pytorch.org/)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Datasets](https://huggingface.co/docs/datasets/)

---

## ğŸ“® è”ç³»æ–¹å¼

- é¡¹ç›®é“¾æ¥: [https://github.com/kleene123/myclasswork](https://github.com/kleene123/myclasswork)
- é—®é¢˜åé¦ˆ: [GitHub Issues](https://github.com/kleene123/myclasswork/issues)

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™å®ƒä¸€ä¸ª â­ï¸**

Made with â¤ï¸ for deep learning education

</div>