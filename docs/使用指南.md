# Transformer 模型剪枝和量化 - 使用指南

## 目录

1. [快速开始](#快速开始)
2. [环境安装](#环境安装)
3. [项目结构](#项目结构)
4. [模块使用说明](#模块使用说明)
5. [实验运行指南](#实验运行指南)
6. [API 文档](#api-文档)
7. [常见问题](#常见问题)

---

## 快速开始

### 1. 克隆项目

```bash
git clone https://github.com/kleene123/myclasswork.git
cd myclasswork
```

### 2. 安装依赖

```bash
pip install -r requirements.txt
```

### 3. 运行基线训练

```bash
python experiments/train_baseline.py --config configs/base_config.yaml
```

### 4. 运行剪枝实验

```bash
python experiments/experiment_pruning.py --unstructured
```

### 5. 运行量化实验

```bash
python experiments/experiment_quantization.py --dynamic
```

### 6. 查看结果

```bash
python experiments/compare_results.py
```

---

## 环境安装

### 系统要求

- Python 3.8 或更高版本
- （可选）CUDA 11.0+ 用于 GPU 加速

### 安装步骤

#### 方法 1：使用 pip

```bash
# 创建虚拟环境（推荐）
python -m venv venv
source venv/bin/activate  # Linux/Mac
# 或
venv\Scripts\activate  # Windows

# 安装依赖
pip install -r requirements.txt
```

#### 方法 2：使用 conda

```bash
# 创建 conda 环境
conda create -n transformer-pruning python=3.9
conda activate transformer-pruning

# 安装 PyTorch（根据您的 CUDA 版本）
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# 安装其他依赖
pip install transformers datasets numpy pandas matplotlib seaborn pyyaml scikit-learn tqdm jupyter
```

#### 方法 3：开发模式安装

```bash
pip install -e .
```

### 验证安装

```python
import torch
import transformers
import datasets

print(f"PyTorch version: {torch.__version__}")
print(f"Transformers version: {transformers.__version__}")
print(f"Datasets version: {datasets.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

---

## 项目结构

```
transformer-pruning-quantization/
├── src/                      # 源代码
│   ├── models/              # 模型定义
│   ├── pruning/             # 剪枝模块
│   ├── quantization/        # 量化模块
│   ├── utils/               # 工具函数
│   └── training/            # 训练模块
├── experiments/             # 实验脚本
├── configs/                 # 配置文件
├── notebooks/               # Jupyter notebooks
├── docs/                    # 文档
└── tests/                   # 测试
```

---

## 模块使用说明

### 1. 数据加载模块

#### 加载标准数据集

```python
from src.utils.data_loader import load_dataset, create_dataloader

# 加载 IMDB 数据集
train_dataset, val_dataset, test_dataset = load_dataset(
    dataset_name="imdb",
    tokenizer_name="bert-base-uncased",
    max_length=128
)

# 创建数据加载器
train_loader = create_dataloader(train_dataset, batch_size=32, shuffle=True)
```

#### 使用 DataLoader 类

```python
from src.utils.data_loader import DataLoader

config = {
    'dataset': 'imdb',
    'batch_size': 32,
    'max_length': 128
}

data_loader = DataLoader(config)
data_loader.prepare_data(tokenizer_name='bert-base-uncased')

train_loader = data_loader.get_train_loader()
```

### 2. 模型模块

#### 使用 BERT 模型

```python
from src.models.bert_wrapper import BERTWrapper

# 初始化模型
model = BERTWrapper(
    model_name='bert-base-uncased',
    num_labels=2
)

# 前向传播
outputs = model(input_ids, attention_mask=attention_mask)

# 获取模型信息
num_params = model.get_num_parameters()
model_size = model.get_model_size()
```

#### 使用自定义 Transformer

```python
from src.models.transformer import TransformerModel

model = TransformerModel(
    vocab_size=30522,
    d_model=512,
    num_heads=8,
    num_layers=6,
    num_classes=2
)
```

### 3. 剪枝模块

#### 结构化剪枝

```python
from src.pruning.structured_pruning import StructuredPruning

# 配置
pruning_config = {
    'attention_head_pruning': {
        'enabled': True,
        'pruning_method': 'importance'
    },
    'ffn_pruning': {
        'enabled': True,
        'intermediate_sparsity': 0.3
    }
}

# 应用剪枝
pruner = StructuredPruning(model, pruning_config)
stats = pruner.apply_pruning(
    dataloader=calibration_loader,
    prune_heads=True,
    num_heads_to_prune=12,
    prune_ffn=True,
    ffn_sparsity=0.3
)
```

#### 非结构化剪枝

```python
from src.pruning.unstructured_pruning import UnstructuredPruning

pruner = UnstructuredPruning(model)
stats = pruner.apply_pruning(
    sparsity=0.5,  # 50% 稀疏度
    global_pruning=True
)

# 查看当前稀疏度
current_sparsity = pruner.get_sparsity()
```

#### 渐进式剪枝

```python
from src.pruning.progressive_pruning import ProgressivePruning

config = {
    'initial_sparsity': 0.0,
    'final_sparsity': 0.5,
    'num_iterations': 5
}

pruner = ProgressivePruning(model, config)

# 查看剪枝计划
schedule = pruner.get_pruning_schedule()

# 应用渐进式剪枝
history = pruner.apply_progressive_pruning(
    trainer=trainer,
    eval_dataloader=val_loader,
    global_pruning=True
)
```

### 4. 量化模块

#### 训练后量化 (PTQ)

```python
from src.quantization.ptq import PostTrainingQuantization

# 动态量化
ptq = PostTrainingQuantization(model)
quantized_model = ptq.apply_dynamic_quantization(dtype=torch.qint8)

# 静态量化
ptq = PostTrainingQuantization(model, config)
quantized_model = ptq.apply_static_quantization(calibration_loader)

# 对比模型大小
comparison = ptq.compare_with_fp32(original_model)
```

#### 量化感知训练 (QAT)

```python
from src.quantization.qat import QuantizationAwareTraining

qat = QuantizationAwareTraining(model, config)

# 完整 QAT 流程
quantized_model, history = qat.apply_qat(
    train_dataloader=train_loader,
    optimizer=optimizer,
    criterion=criterion,
    num_epochs=3,
    device=device
)
```

#### 混合精度量化

```python
from src.quantization.mixed_precision import MixedPrecisionQuantization

mp = MixedPrecisionQuantization(model, config)

# 分析层敏感度
sensitivity = mp.analyze_layer_sensitivity(
    dataloader=val_loader,
    criterion=criterion,
    num_samples=100
)

# 选择敏感层
sensitive_layers = mp.select_sensitive_layers(top_k=5)

# 应用混合精度
layer_config = mp.apply_mixed_precision(sensitive_layers)

# 估算模型大小
size_stats = mp.estimate_model_size()
```

### 5. 训练和评估模块

#### 训练器

```python
from src.training.trainer import Trainer

config = {
    'learning_rate': 2e-5,
    'epochs': 3,
    'max_grad_norm': 1.0
}

trainer = Trainer(model, config, device)

# 训练
history = trainer.train(
    train_loader=train_loader,
    val_loader=val_loader,
    evaluator=evaluator
)

# 保存检查点
trainer.save_checkpoint('checkpoints/model.pt', epoch=3)
```

#### 评估器

```python
from src.training.evaluator import Evaluator

evaluator = Evaluator(model, device)

# 评估
metrics = evaluator.evaluate(test_loader)

# 获取预测
predictions = evaluator.predict(test_loader)

# 详细评估（包含混淆矩阵等）
detailed_metrics = evaluator.evaluate_detailed(test_loader)

# 比较两个模型
comparison = evaluator.compare_models(
    other_model=quantized_model,
    dataloader=test_loader
)
```

### 6. 工具模块

#### 计算指标

```python
from src.utils.metrics import (
    compute_metrics,
    ModelSizeCalculator,
    InferenceTimer,
    calculate_sparsity
)

# 计算分类指标
metrics = compute_metrics(predictions, labels)

# 计算模型大小
size_info = ModelSizeCalculator.get_model_size(model)

# 比较两个模型
comparison = ModelSizeCalculator.compare_models(model1, model2)

# 测量推理速度
timer = InferenceTimer(model, device)
time_stats = timer.measure_inference_time(test_loader, num_batches=100)

# 计算稀疏度
sparsity_stats = calculate_sparsity(model)
```

#### 可视化

```python
from src.utils.visualization import (
    plot_training_curves,
    plot_comparison,
    plot_sparsity_vs_accuracy,
    create_results_table
)

# 绘制训练曲线
plot_training_curves(
    history={'loss': [...], 'accuracy': [...]},
    save_path='results/training_curves.png'
)

# 绘制对比图
plot_comparison(
    results=[...],
    metrics=['accuracy', 'f1', 'model_size_mb'],
    save_path='results/comparison.png'
)

# 创建结果表格
df = create_results_table(
    results=[...],
    save_path='results/table.csv'
)
```

---

## 实验运行指南

### 实验 1：训练基线模型

```bash
# 使用默认配置
python experiments/train_baseline.py

# 使用自定义配置
python experiments/train_baseline.py --config my_config.yaml
```

**配置文件说明**（`configs/base_config.yaml`）：
```yaml
model:
  name: "bert-base-uncased"  # 模型名称
  num_labels: 2              # 分类类别数

training:
  batch_size: 32             # 批次大小
  learning_rate: 2e-5        # 学习率
  epochs: 3                  # 训练轮数
```

### 实验 2：剪枝实验

```bash
# 运行所有剪枝实验
python experiments/experiment_pruning.py

# 只运行结构化剪枝
python experiments/experiment_pruning.py --structured

# 只运行非结构化剪枝
python experiments/experiment_pruning.py --unstructured

# 只运行渐进式剪枝
python experiments/experiment_pruning.py --progressive

# 使用自定义配置
python experiments/experiment_pruning.py --config my_pruning_config.yaml
```

**配置文件说明**（`configs/pruning_config.yaml`）：
```yaml
pruning:
  method: "structured"       # 剪枝方法
  sparsity: 0.5             # 目标稀疏度
  
  attention_head_pruning:
    enabled: true
    num_heads_to_prune: 4   # 要剪枝的头数量
  
  unstructured:
    method: "magnitude"      # 幅度剪枝
    sparsity_levels: [0.1, 0.3, 0.5, 0.7, 0.9]
```

### 实验 3：量化实验

```bash
# 运行所有量化实验
python experiments/experiment_quantization.py

# 只运行动态量化
python experiments/experiment_quantization.py --dynamic

# 只运行静态量化
python experiments/experiment_quantization.py --static

# 只运行QAT
python experiments/experiment_quantization.py --qat
```

### 实验 4：组合实验

```bash
# 运行剪枝+量化组合实验
python experiments/experiment_combined.py
```

### 实验 5：结果对比

```bash
# 生成对比图表和报告
python experiments/compare_results.py

# 指定结果目录
python experiments/compare_results.py --results-dir ./results --output-dir ./results/comparison
```

---

## API 文档

### 核心类和方法

#### BERTWrapper

```python
class BERTWrapper(nn.Module):
    """BERT 模型封装类"""
    
    def __init__(self, model_name: str, num_labels: int, config: Dict = None):
        """初始化 BERT 模型"""
        
    def forward(self, input_ids, attention_mask=None, labels=None):
        """前向传播"""
        
    def get_num_parameters(self, trainable_only: bool = False) -> int:
        """获取参数数量"""
        
    def get_model_size(self) -> float:
        """获取模型大小（MB）"""
```

#### StructuredPruning

```python
class StructuredPruning:
    """结构化剪枝"""
    
    def __init__(self, model, config: Dict = None):
        """初始化"""
        
    def apply_pruning(
        self,
        dataloader=None,
        prune_heads: bool = True,
        num_heads_to_prune: int = 0,
        prune_ffn: bool = True,
        ffn_sparsity: float = 0.0
    ) -> Dict:
        """应用剪枝"""
```

#### UnstructuredPruning

```python
class UnstructuredPruning:
    """非结构化剪枝"""
    
    def __init__(self, model, config: Dict = None):
        """初始化"""
        
    def apply_pruning(self, sparsity: float, global_pruning: bool = True) -> Dict:
        """应用剪枝"""
        
    def get_sparsity(self) -> float:
        """获取当前稀疏度"""
```

#### PostTrainingQuantization

```python
class PostTrainingQuantization:
    """训练后量化"""
    
    def __init__(self, model, config: Dict = None):
        """初始化"""
        
    def apply_dynamic_quantization(self, dtype=torch.qint8):
        """应用动态量化"""
        
    def apply_static_quantization(self, calibration_dataloader):
        """应用静态量化"""
```

---

## 常见问题

### Q1: 如何选择合适的稀疏度？

**A**: 建议从小稀疏度开始逐步增加：
- 10-30%: 通常几乎没有性能损失
- 30-50%: 可能有轻微性能下降
- 50-70%: 性能下降明显，需要仔细微调
- 70-90%: 极端稀疏，性能大幅下降

### Q2: 量化后模型变大了怎么办？

**A**: 这可能是因为：
1. 使用了动态量化，只量化权重
2. 模型保存时包含了额外的元数据
3. 解决方案：
   - 使用静态量化
   - 使用专门的量化模型保存格式
   - 使用模型转换工具（如 ONNX）

### Q3: 训练时内存不足怎么办？

**A**: 尝试以下方法：
1. 减小 batch_size
2. 使用梯度累积
3. 使用混合精度训练
4. 减小 max_length
5. 使用更小的模型（如 DistilBERT）

```python
# 梯度累积示例
accumulation_steps = 4
for i, batch in enumerate(train_loader):
    loss = model(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### Q4: 如何在 CPU 上运行？

**A**: 项目自动检测设备，在没有 GPU 时会使用 CPU：

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

如果想强制使用 CPU：

```python
device = torch.device('cpu')
model = model.to(device)
```

### Q5: 如何使用自己的数据集？

**A**: 修改 `data_loader.py`：

```python
from src.utils.data_loader import TextClassificationDataset

# 准备数据
texts = [...]  # 文本列表
labels = [...]  # 标签列表

# 创建数据集
dataset = TextClassificationDataset(
    texts=texts,
    labels=labels,
    tokenizer=tokenizer,
    max_length=128
)
```

### Q6: 剪枝后如何保存模型？

**A**: 剪枝后的模型可以直接保存：

```python
# 保存整个模型
torch.save(model.state_dict(), 'pruned_model.pt')

# 保存 BERT 模型
model.save_pretrained('pruned_bert')

# 加载
model.load_state_dict(torch.load('pruned_model.pt'))
```

### Q7: 如何查看模型的稀疏度分布？

**A**: 使用 `calculate_sparsity` 函数：

```python
from src.utils.metrics import calculate_sparsity

stats = calculate_sparsity(model)
print(f"整体稀疏度: {stats['overall_sparsity']:.2%}")

# 查看每层的稀疏度
for name, sparsity in stats['layer_sparsity'].items():
    print(f"{name}: {sparsity:.2%}")
```

### Q8: 实验结果在哪里？

**A**: 实验结果保存在以下位置：
- 模型检查点：`./checkpoints/`
- 实验结果：`./results/`
- 训练输出：`./outputs/`
- 日志：`./logs/`

### Q9: 如何调试代码？

**A**: 使用 Python 调试器：

```python
import pdb; pdb.set_trace()  # 设置断点
```

或使用 IDE 的调试功能（VSCode, PyCharm）

### Q10: 如何贡献代码？

**A**: 
1. Fork 项目
2. 创建特性分支
3. 提交更改
4. 推送到分支
5. 创建 Pull Request

---

## 更多资源

- [PyTorch 官方文档](https://pytorch.org/docs/)
- [Transformers 库文档](https://huggingface.co/docs/transformers/)
- [项目 GitHub 页面](https://github.com/kleene123/myclasswork)
- [理论基础文档](./理论基础.md)
- [实验报告模板](./实验报告模板.md)

---

## 联系方式

如有问题或建议，请通过以下方式联系：
- GitHub Issues
- Email: [待补充]

---

**最后更新**: 2024
