# 项目使用说明

## 快速开始

### 1. 环境准备

#### 系统要求

- Python 3.8 或更高版本
- 推荐使用 Linux 或 macOS（Windows 也支持）
- 至少 16GB RAM
- （可选）NVIDIA GPU with CUDA 支持

#### 安装依赖

```bash
# 克隆仓库
git clone <repository-url>
cd transformer-quantization-report

# 创建虚拟环境（推荐）
python -m venv venv
source venv/bin/activate  # Linux/Mac
# 或
venv\Scripts\activate  # Windows

# 安装依赖
pip install -r requirements.txt

# 或安装开发版本
pip install -e .
```

### 2. 运行实验

#### 方式一：运行所有实验

这是最简单的方式，会自动运行所有实验：

```bash
cd experiments
python run_all_experiments.py
```

#### 方式二：逐个运行实验

如果你想更细致地控制实验过程：

```bash
# 实验1: 训练基线模型（如果有预训练模型可跳过）
python experiments/01_baseline_training.py

# 实验2: 动态量化
python experiments/02_dynamic_quantization.py

# 实验3: 静态量化
python experiments/03_static_quantization.py

# 实验4: 量化感知训练
python experiments/04_qat_experiment.py

# 实验5: 混合精度
python experiments/05_mixed_precision.py

# 实验6: 综合对比（重要！）
python experiments/06_comprehensive_comparison.py
```

#### 方式三：使用命令行工具

如果你使用了 `pip install -e .` 安装：

```bash
# 运行所有实验
run-experiments

# 生成报告
generate-report
```

### 3. 查看结果

#### 结果目录结构

```
results/
├── baseline/              # 基线模型结果
│   ├── model/            # 模型文件
│   └── results.json      # 评估结果
├── dynamic_quant/         # 动态量化结果
├── static_quant/          # 静态量化结果
├── qat/                   # QAT 结果
├── mixed_precision/       # 混合精度结果
└── comparison/            # 综合对比结果（最重要）
    ├── summary_table.csv          # 对比表格
    ├── summary_report.md          # 文字报告
    ├── accuracy_comparison.png    # 准确率对比图
    ├── size_comparison.png        # 模型大小对比图
    ├── speed_comparison.png       # 推理速度对比图
    ├── radar_chart.png           # 雷达图
    ├── accuracy_vs_size.png      # 准确率-大小权衡图
    └── summary_dashboard.png     # 综合仪表板
```

#### 查看对比表格

```bash
# 查看 CSV 文件
cat results/comparison/summary_table.csv

# 或使用 Excel/Google Sheets 打开
```

#### 查看图表

图表文件为 PNG 格式，可以直接用图片浏览器查看。

### 4. 修改配置

#### 修改实验配置

编辑 `configs/experiment_config.yaml`：

```yaml
experiments:
  baseline:
    enabled: true
    model: "bert-base-uncased"  # 可改为其他模型
    dataset: "imdb"
    
evaluation:
  test_samples: 1000  # 减少样本数可加快测试
  batch_size: 32
  num_iterations: 100  # 性能测试迭代次数
```

#### 修改模型配置

编辑 `configs/model_config.yaml`：

```yaml
models:
  bert_base:
    name: "bert-base-uncased"
    max_length: 128  # 最大序列长度
    num_labels: 2    # 分类标签数
```

#### 修改量化配置

编辑 `configs/quantization_config.yaml`：

```yaml
quantization:
  dynamic:
    dtype: "qint8"  # 量化数据类型
    
  static:
    backend: "fbgemm"  # x86 使用 fbgemm，ARM 使用 qnnpack
    calibration:
      num_samples: 1000  # 校准样本数
```

## 高级使用

### 使用自定义数据集

1. 在 `src/utils/data_loader.py` 中添加数据加载函数：

```python
def load_custom_dataset(tokenizer_name, max_length):
    # 实现你的数据加载逻辑
    pass
```

2. 在实验脚本中调用：

```python
train_dataset, test_dataset, tokenizer = load_custom_dataset(...)
```

### 使用不同的模型

修改配置文件或在代码中指定：

```python
bert_model = BERTModel(
    model_name="distilbert-base-uncased",  # 使用 DistilBERT
    num_labels=2
)
```

### 在 GPU 上运行

如果有 NVIDIA GPU：

```bash
# 检查 CUDA 是否可用
python -c "import torch; print(torch.cuda.is_available())"

# 在实验脚本中会自动检测并使用 GPU
# 或手动指定设备
python experiments/02_dynamic_quantization.py --device cuda
```

### 调整测试样本数

为了加快测试（开发阶段）：

```yaml
# configs/experiment_config.yaml
evaluation:
  test_samples: 100  # 减少到 100 个样本
  num_iterations: 10  # 减少性能测试迭代次数
```

### 生成 PDF 报告

需要安装 Pandoc：

```bash
# Ubuntu/Debian
sudo apt-get install pandoc texlive-xetex

# macOS
brew install pandoc
brew install --cask mactex

# 转换为 PDF
pandoc report/课程设计报告.md -o report/课程设计报告.pdf \
  --pdf-engine=xelatex \
  -V CJKmainfont="SimSun"
```

## 常见问题

### Q: 运行实验时出现内存不足

**A**: 减少批次大小和测试样本数：

```yaml
# configs/experiment_config.yaml
evaluation:
  test_samples: 500
  batch_size: 16
```

### Q: 动态量化后模型大小没有变化

**A**: 动态量化主要量化权重，需要保存模型文件才能看到大小变化。在内存中，由于还保留了浮点激活值，大小变化可能不明显。

### Q: 静态量化失败

**A**: 静态量化对模型结构有要求，Transformer 模型可能不完全支持。尝试：
- 使用动态量化或 QAT
- 减少校准样本数
- 检查 PyTorch 版本（推荐 2.0+）

### Q: 如何跳过已完成的实验

**A**: 直接运行后续实验即可，实验脚本会自动加载已有模型。

### Q: 推理速度没有提升

**A**: 可能原因：
- 在 CPU 上，INT8 加速需要硬件支持
- 模型太小，量化开销相对较大
- 使用了动态量化而非静态量化

### Q: 准确率下降太多

**A**: 尝试：
- 使用 QAT 而非静态量化
- 增加校准样本数
- 使用 FP16 而非 INT8
- 检查数据预处理是否正确

## 性能优化建议

### 1. 使用 GPU

GPU 可以显著加快训练和推理：

```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
```

### 2. 减少测试样本

开发阶段使用少量样本快速迭代：

```yaml
evaluation:
  test_samples: 100
```

### 3. 并行数据加载

```python
test_loader = DataLoader(
    test_dataset,
    batch_size=32,
    num_workers=4  # 使用多进程
)
```

### 4. 混合精度训练

使用 PyTorch 的自动混合精度：

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

## API 文档

### 核心模块

#### BERTModel

```python
from src.models.bert_model import BERTModel

# 创建模型
model = BERTModel(
    model_name="bert-base-uncased",
    num_labels=2,
    cache_dir="./models/cache"
)

# 保存模型
model.save("path/to/save")

# 加载模型
model = BERTModel.load("path/to/load")

# 预测
predictions = model.predict(texts, batch_size=32)
```

#### 动态量化

```python
from src.quantization.dynamic_quantization import apply_dynamic_quantization

quantized_model = apply_dynamic_quantization(
    model,
    dtype=torch.qint8
)
```

#### 评估

```python
from src.evaluation.accuracy_eval import evaluate_accuracy
from src.evaluation.performance_eval import measure_inference_time
from src.evaluation.size_eval import calculate_model_size

# 准确率
metrics = evaluate_accuracy(model, test_loader, device='cpu')

# 推理时间
timing = measure_inference_time(model, test_loader, device='cpu')

# 模型大小
size_mb = calculate_model_size(model)
```

## 获取帮助

- 查看日志文件：`logs/`
- 查看源代码文档字符串
- 提交 Issue：[项目 Issues 页面]

## 许可证

MIT License
